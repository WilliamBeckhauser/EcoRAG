# EcoRAG

**EcoRAG**
The EcoRAG framework, combines RAG through KGs (Graph-RAG) with an iterative validation mechanism based on ICL. The method is structured into three main phases: (i) Pre-production: responsible for organizing and structuring domain knowledge; (ii) Prompt Raising: which builds enriched prompts from the KG; and (iii) ICL-Based Validation: where an automated module performs checks and issues corrective textual feedback. This iterative validation is integrated into the modelâ€™s contextual memory, allowing for successive improvements in the responses without changing the LLMâ€™s parameters or resorting to fine-tuning techniques.


---

## âš™ï¸ How It Works

### 1. Task Processing Workflow

1. **Data Loading**: Retrieves data from a PostgreSQL database.
2. **Task Execution**:
   - Applies task-specific prompt templates.
   - Sends requests to the language model.
   - Validates responses based on task criteria.
3. **Performance Monitoring**:
   - Measures latency, throughput, token usage.
   - Monitors CPU, memory, and energy usage.
   - Calculates FLOPS per token.
4. **Result Storage**: Saves all metrics and responses into PostgreSQL for later analysis.

### 2. Supported Tasks

- **Sentiment Analysis** â€“ Positive, negative, neutral classification  
- **Medical Information Extraction** â€“ Identifies symptoms, conditions, treatments  
- **Review Classification** â€“ Classifies review content based on predefined labels  
- **Industrial Machinery Information** â€“ Extracts technical data and operational context  
- **Fake News Detection** â€“ Detects misinformation using evidence-based reasoning  

### 3. Performance Metrics

EcoRAG tracks:

- â±ï¸ **Latency** â€“ Inference time  
- âš¡ **Energy Consumption** â€“ Joules consumed  
- ğŸ’¾ **Memory Usage** â€“ RAM utilization  
- ğŸ“ˆ **FLOPS per Token** â€“ Efficiency metric  
- ğŸ” **Throughput** â€“ Tokens per second  
- ğŸ§  **Token Count** â€“ Total tokens processed  

---

## ğŸš€ Features

- Modular architecture for easy extension  
- Batch processing for scalability  
- PostgreSQL and Neo4j integration  
- Graph-based reasoning (optional)  
- Works with local models (via Ollama)  
- Customizable prompt templates for each task  

---

## ğŸ“¦ Installation

1. **Clone the repository**

```bash
git clone https://github.com/WilliamBeckhauser/ecorag.git
cd ecorag
```

2. **Create and activate a virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Create a `.env` file**

```env
# Ollama LLM settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b

# PostgreSQL connection
POSTGRES_DBNAME=your_dbname
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# Experiment schema/table
EXPERIMENT_SCHEMA=experiments
EXPERIMENT_TABLE=results
```

---

## âš™ï¸ Advanced Setup (Data Split + Graph RAG)

### 1. Environment Configuration (.env)

Create a `.env` file (or update the existing one) with the following variables:

```env
# API Tokens (set only if necessary)
OPENROUTER_TOKEN="if necessary"
API_KEY_OPENAI="if necessary"
API_KEY_GOOGLE="if necessary"
# Ollama does not require any token here

# PostgreSQL credentials (substitua pelos seus dados)
POSTGRES_DBNAME="postgres"
POSTGRES_USER="postgres"
POSTGRES_PASSWORD="postgres"
POSTGRES_HOST="localhost"
POSTGRES_PORT="5432"

# Neo4j credentials (substitua pelos seus dados)
NEO4J_URI=""
NEO4J_USER=""
NEO4J_PASSWORD=""
```

### 2. Data Preparation

After configuring the `.env` file, run the following command to clone the datasets (sst5, gossipcop, and politifact) and create schemas to separate the data into validation, test, original, experiments, and graphs:

```bash
python src/utils/data_split.py --z 2.576 --p 0.5 --e 0.02
```

### 3. Graph Creation

Next, create the graphs. The code currently uses the ollama3.1 8b model via Ollama. Customize the parameters as needed and run:

```bash
python src/utils/graph_creator.py \
  --neo4j_uri "your_uri" \
  --neo4j_user "neo4j" \
  --neo4j_password "your_passworw" \
  --postgres_schema "graph" \
  --postgres_table "tablename" \
  --id_column "id" \
  --title_column "title" \
  --classification_column "label"
```

### 4. Running the Graph RAG Experiment

After configuring or creating the graphs, run the following command to execute the experiment using graph data defined in the `.env` file. In this example, EcoRAG is used for fake news detection:

```bash
python ecorag.py \
  --source_schema test \
  --table_name politifact \
  --review_column title \
  --label_column label \
  --model_name llama3.1:8b \
  --provider ollama \
  --code_name ecorag \
  --execution_mode ecorag \
  --task fake_news
```

---

## ğŸ“ Project Structure

```
ecorag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # Loads env settings
â”‚   â”œâ”€â”€ models/                 # (Reserved for future model code)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ database.py         # PostgreSQL operations
â”‚   â”‚   â”œâ”€â”€ performance.py      # Metrics and system monitoring
â”‚   â”‚   â”œâ”€â”€ data_split.py       # Dataset loading and splitting
â”‚   â”‚   â””â”€â”€ graph_creator.py    # Neo4j graph building
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â””â”€â”€ llm_provider.py     # Interfaces with LLM APIs
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ experiment.py       # Core experiment logic
â”‚   â””â”€â”€ main.py                 # CLI entry point for experiments
â”œâ”€â”€ .env                        # Configuration file
â”œâ”€â”€ ecorag.py                   # Main experiment runner
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```
